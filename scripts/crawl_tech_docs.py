import os
import time
import trafilatura
from urllib.parse import urlparse

# --- C·∫§U H√åNH ---
OUTPUT_DIR = "./real_tech_dataset"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Danh s√°ch 10 URL ch·∫•t l∆∞·ª£ng cao (Selected Sources)
# ƒê√¢y l√† c√°c trang ƒë∆∞·ª£c d√¢n IT tin d√πng, n·ªôi dung r·∫•t s√¢u v√† chu·∫©n.
TARGET_URLS = {
    "Microservices": "https://microservices.io/patterns/microservices.html", # Ngu·ªìn g·ªëc c·ªßa kh√°i ni·ªám Microservices
    "Docker_Overview": "https://docs.docker.com/get-started/overview/", # Official Docker Docs
    "Apache_Kafka": "https://kafka.apache.org/intro", # Official Kafka Intro
    "REST_API": "https://aws.amazon.com/what-is/restful-api/", # AWS gi·∫£i th√≠ch r·∫•t k·ªπ v·ªÅ REST
    "CI_CD": "https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment", # Atlassian gi·∫£i th√≠ch c·ª±c hay
    "SQL_vs_NoSQL": "https://www.mongodb.com/resources/basics/databases/nosql-explained", # MongoDB so s√°nh chi ti·∫øt
    "OAuth2_Auth": "https://auth0.com/intro-to-iam/what-is-oauth-2", # Auth0 gi·∫£i th√≠ch v·ªÅ Auth
    "Kubernetes_K8s": "https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/", # Official K8s Docs
    "SOLID_Principles": "https://www.digitalocean.com/community/conceptual-articles/s-o-l-i-d-the-first-five-principles-of-object-oriented-design", # DigitalOcean gi·∫£i th√≠ch SOLID
    "Git_Version_Control": "https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control" # Ch∆∞∆°ng 1 c·ªßa s√°ch Pro Git
}

def crawl_and_save():
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu t·ª´ {len(TARGET_URLS)} ngu·ªìn...\n")
    
    success_count = 0
    
    for topic, url in TARGET_URLS.items():
        print(f"‚è≥ ƒêang t·∫£i: {topic} ...")
        print(f"   Source: {url}")
        
        try:
            # 1. T·∫£i HTML v·ªÅ
            downloaded = trafilatura.fetch_url(url)
            
            if downloaded is None:
                print(f"   ‚ùå L·ªói: Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c URL (C√≥ th·ªÉ do ch·∫∑n Bot).")
                continue

            # 2. Tr√≠ch xu·∫•t n·ªôi dung ch√≠nh (Main Content Extraction)
            # include_tables=True: L·∫•y c·∫£ b·∫£ng so s√°nh (r·∫•t t·ªët cho RAG)
            text_content = trafilatura.extract(downloaded, include_tables=True, include_comments=False)
            
            if text_content:
                # 3. Th√™m Metadata v√†o ƒë·∫ßu file ƒë·ªÉ LightRAG hi·ªÉu context
                header = f"Topic: {topic}\nSource: {url}\nDomain: Software Engineering/Backend\n"
                header += "="*40 + "\n\n"
                
                final_content = header + text_content
                
                # 4. L∆∞u file
                filename = os.path.join(OUTPUT_DIR, f"{topic}.txt")
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(final_content)
                
                # T√≠nh s∆° b·ªô ƒë·ªô d√†i
                word_count = len(text_content.split())
                print(f"   ‚úÖ Th√†nh c√¥ng! ƒê√£ l∆∞u {word_count} t·ª´ v√†o file '{topic}.txt'")
                success_count += 1
            else:
                print(f"   ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ {url}")
                
        except Exception as e:
            print(f"   ‚ùå Exception: {e}")
        
        print("-" * 50)
        # Ngh·ªâ 1 x√≠u ƒë·ªÉ kh√¥ng b·ªã ch·∫∑n IP
        time.sleep(1) 

    print(f"\nüéâ Ho√†n t·∫•t! ƒê√£ l·∫•y ƒë∆∞·ª£c {success_count}/{len(TARGET_URLS)} t√†i li·ªáu.")
    print(f"üìÇ Ki·ªÉm tra th∆∞ m·ª•c: {os.path.abspath(OUTPUT_DIR)}")

if __name__ == "__main__":
    crawl_and_save()